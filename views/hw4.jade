extends layout

block content
  h1= title
  p Identify 3 design decisions that were made by the creators of the Zeno R25. Discuss the impact these choices have on the robot's ability to interact.
  p One of the more glaring design decisions is that Zeno R25 has a human-like head, but the body of a robot. The head appearing like a human allows Zeno R25 to mimic human emotions more accurately. Accordingly, the face risks appearing “too human,” which may be why the creators decided to give the body of a robot. This obviously artificial body provides contrast to  the head, helping to keep it from being too human. Keeping Zeno R25 out of the uncanny valley allows people to focus and interpret the expressions of the robot’s face, rather than focus on how bizarre the robot’s facial movements are.
  p Another design choice was to keep the height of Zeno R25 relatively short, about 22 inches. While Zeno R25 is supposed to look like a child, he isn’t much bigger than a baby. This ensures that the robot is rarely taller than it’s audience so it doesn’t appear threatening. RoboKind hinted that some applications for Zeno R25 in the future are reading books, teaching languages, and other activities that seem to cater more to children. If this is the case, an advantage of being smaller is that people may tend to treat the robot as less like a peer and more like a toy. This is possibly a more suitable interaction for children. However, for adults, the smaller stature may warrant the perception of Zeno R25 as more servile and submissive, as explained in the creation of The Snackbot http://www.cs.cmu.edu/~mklee/materials/Publication/2009-HRI-robot_design_process.pdf.
  p Finally, RoboKind has equipped Zeno R25 with an LED touchscreen in the middle of its chest. The touchscreen is nice because it allows for a quick response from the human, much like normal conversation. While putting this device in the middle of Zeno R25’s chest may seem odd and throw off the humanness of the interaction by putting such an overtly machine-like interactor in plain sight, there doesn’t seem to be any other option for preserving the quality of the interaction. This allows people to continue facing the robot when providing their input again mirrors human to human communication. Further, inputting at Zeno R25’s heart feels like you’re giving the information to Zeno R25, whereas if the input was on it’s leg, it might feel like you’re giving the information for Zeno R25 to use.
  p Pick a robot from the following list (or another socially interactive/expressive humanlike robot) and identify 3 DIFFERENT design decisions and their impacts. How do this robot's possible interactions differ from the R25? You are encouraged to search for additional videos in order to see what movements and sensing they are capable of.

  p Aldebaran Robotics’ NAO robot is highly customizable and programmable to users’ needs and intended uses. For example, NAO can be used as teaching assistant in classrooms, a language assistant for people with speech problems, or as a child’s companion. Aldebaran’s movement libraries (like Choregraphe) allow users to create specific movements and tailored behaviors for their robots. Aldebaran’s open source platform has helped build a NAO community of developers, researchers, teachers, expanding the applications of humanoid robotics in more fields (such as education, face and language recognition, signal processing, human-robot interaction, etc.) Conversely, though the R25 has software and frameworks that users can build on, users aren’t as able to contribute to the framework in an open source manner. This difference in code base could make R25 a more advanced, refined system, but less accessible to the community.
  p NAO has a hard exterior, making it more durable but also limiting its range of expression. One of the R25’s key features is its full range of facial expression, which NAO lacks because of its set facial exterior. This makes the R25 much more human-like than NAO, as people can directly interact with and respond to the R25’s facial expressions, reducing the social distance between people and robots. The design choice to make NAO’s face a set expression (though an expression that is cute and friendly) might create a kind of social distance between people and NAO, so that people can interact with NAO using human behaviors but also maintain the obvious knowledge that NAO is a robot and and an assistant. 
  p What NAO lacks in fine facial expression, it makes up for in 25 degrees of freedom for movement and a sensor network that includes multiple vision, audio, touch, and pressure sensors, thus allowing NAO to exhibit finer human-like body language and behaviors. NAO is also designed to be small and look cute and friendly, making it more accessible and acceptable for use in settings with children (as in a classroom or a home). The R25 has 21 degrees of freedom, making it slightly less adaptable to its environment (https://www.youtube.com/watch?v=2STTNYNF4lk is an example of NAO’s wider range of movement than the R25). However, one interesting difference is that the R25 has 8 microphones while NAO only has 4, making the R25 much more sensitive to its sound environment, and also more finetune and efficient in its language and music processing.
  p NAO supports WiFi and Ethernet, and can connect to objects and networks in its environment. Wireless access allows users to pilot and program NAO using any computer on the network, making NAO more portable and adaptable to more environments. One application that utilizes NAO’s wireless connectivity is remote audio signal processing, which is often faster and more efficient than embedded processing. Though users can also process audio signals directly in the robot, the NAOqi framework uses Simple Object Access Protocol (SOAP) to send and receive audio signals over the Internet to a processing platform. The R25 also supports WiFi and Ethernet, but users aren’t as able to utilize it for remote applications as with NAO, because of the R25’s less open source platform, making the R25 less customizable and portable.
  p List at least 2 abilities that the R25 does not have that would have been useful for expressing emotion. How would these abilities have helped you with your animation designs? What are some of the reasons these abilities would be difficult to implement?
  p It looks like the R25 doesn’t have the ability to adjust eye pitch, which would have been useful to keep it’s eyes level when adjusting the neck pitch. While there is an eye_vert channel, adjusting it didn’t cause the robots eyes to move at all. While I don’t think this would be particularly difficult to implement, it might be the case that it would complicate the eye movement mechanisms and require space for another motor inside the head. The ability adjust position of the lips in ways besides bending them up and down would also make it easier to accurately express emotion. For example, the R25 could sneer, or show its teeth more during a smile. I’d imagine that this would require the connection between the skin and the “muscle” underneath to have at least one more degree of freedom, which might be difficult to implement, especially considering space constraints.

  p Mini-study:
  img(src="/images/p1.png" alt="")
  img(src="/images/p2.png" alt="")
  img(src="/images/p3.png" alt="")
  br
  a(href=code) Code
